# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KhRelbT-8ZaD83Sop7lTtR6ZMFzbmJIM
"""

!pip install transformers sentence-transformers faiss-cpu

import os, re, string, json, time
from pathlib import Path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight

from sentence_transformers import SentenceTransformer
import faiss
import joblib

CSV_PATH = "/content/hasil_scraping_TIKTOK.csv"
TEXT_COL = "content"
SCORE_COL = "score"
OUT_DIR = Path("repo_output")
OUT_DIR.mkdir(parents=True, exist_ok=True)

EMBED_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
RANDOM_SEED = 42
TEST_SIZE = 0.2
VAL_SIZE = 0.1
N_NEIGHBORS = 5

print("Loading CSV:", CSV_PATH)
df = pd.read_csv(CSV_PATH)
assert TEXT_COL in df.columns and SCORE_COL in df.columns, "CSV harus punya kolom 'content' dan 'score'"
df = df[[TEXT_COL, SCORE_COL]].dropna().reset_index(drop=True)
print("Rows:", len(df))
print(df.head(3))

def clean_text(s):
    s = str(s).lower()
    s = re.sub(r"http\S+|www\.\S+", " ", s)
    s = re.sub(r"\d+", " ", s)
    s = s.translate(str.maketrans("", "", string.punctuation))
    s = re.sub(r"\s+", " ", s).strip()
    return s

df["text_clean"] = df[TEXT_COL].apply(clean_text)

def map_score_to_sentiment(score):
    try:
        s = int(score)
    except:
        return "Netral"
    if s <= 2:
        return "Negatif"
    elif s == 3:
        return "Netral"
    else:
        return "Positif"

df["label_str"] = df[SCORE_COL].apply(map_score_to_sentiment)
print("\nLabel distribution:\n", df["label_str"].value_counts())

le = LabelEncoder()
y = le.fit_transform(df["label_str"])
label_names = list(le.classes_)
print("Label names:", label_names)

print("\nLoading embed model:", EMBED_MODEL)
embedder = SentenceTransformer(EMBED_MODEL)
texts = df["text_clean"].tolist()

print("Computing embeddings (may take time)...")
embeddings = embedder.encode(texts, convert_to_numpy=True, show_progress_bar=True)
embeddings = embeddings.astype("float32")
print("Embeddings shape:", embeddings.shape)

X_train, X_temp, y_train, y_temp, idx_train, idx_temp = train_test_split(
    embeddings, y, df.index.values, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=y
)
# split temp -> val + test
val_count = int(len(X_temp) * (1 - VAL_SIZE))
X_val, X_test = X_temp[:val_count], X_temp[val_count:]
y_val, y_test = y_temp[:val_count], y_temp[val_count:]
idx_val, idx_test = idx_temp[:val_count], idx_temp[val_count:]

print("\nSplits:", X_train.shape[0], "train,", X_val.shape[0], "val,", X_test.shape[0], "test")

classes = np.unique(y_train)
class_weights = compute_class_weight(class_weight="balanced", classes=classes, y=y_train)
class_weight_dict = {int(c): float(w) for c, w in zip(classes, class_weights)}
print("Class weights:", class_weight_dict)

clf = LogisticRegression(max_iter=2000, class_weight=class_weight_dict)
print("Training classifier...")
clf.fit(X_train, y_train)

# Save classifier + encoder
joblib.dump(clf, OUT_DIR / "clf_logreg.joblib")
joblib.dump(le, OUT_DIR / "label_encoder.joblib")
print("Saved classifier ->", OUT_DIR / "clf_logreg.joblib")

y_val_pred = clf.predict(X_val)
y_test_pred = clf.predict(X_test)
print("\nValidation report:\n", classification_report(y_val, y_val_pred, target_names=label_names))
print("\nTest report:\n", classification_report(y_test, y_test_pred, target_names=label_names))

cm = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(5,4))
plt.imshow(cm, cmap="Blues", interpolation="nearest")
plt.colorbar()
plt.xticks(range(len(label_names)), label_names, rotation=45)
plt.yticks(range(len(label_names)), label_names)
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, cm[i,j], ha="center", va="center", color="white" if cm[i,j]>cm.max()/2 else "black")
plt.title("Confusion matrix (test)")
plt.tight_layout()
plt.savefig(OUT_DIR / "confusion_matrix_test.png", dpi=150)
plt.show()

d = embeddings.shape[1]
index = faiss.IndexFlatL2(d)
index.add(embeddings)

def search_similar(text, top_k=5):
    emb = embedder.encode([clean_text(text)], convert_to_numpy=True).astype("float32")
    D, I = index.search(emb, top_k)
    rows = []
    for dist, idx in zip(D[0], I[0]):
        rows.append({
            "index": int(idx),
            "text": df.loc[int(idx), TEXT_COL],
            "score": int(df.loc[int(idx), SCORE_COL]),
            "label": df.loc[int(idx), "label_str"],
            "distance": float(dist)
        })
    return rows

def predict_with_probs(text):
    text_clean = clean_text(text)
    emb = embedder.encode([text_clean], convert_to_numpy=True).astype("float32")
    probs = clf.predict_proba(emb)[0]
    return {label_names[i]: float(probs[i]) for i in range(len(label_names))}

def predict_and_print(text):
    scores = predict_with_probs(text)
    top_label = max(scores, key=scores.get)
    ms = int((time.time()*1000) % 100)

    print(f"\nTulis Ulasan: {text}")
    print(f"1/1 [==============================] - 0s {ms}ms/step")
    print(f"Kalimat :  {top_label}")

    order = ["Negatif", "Netral", "Positif"]
    for lbl in order:
        val = scores.get(lbl, 0.0)
        print(f"{lbl:<7}:  {val:.7f}")

    print("\nNearest neighbors dari dataset:")
    for r in search_similar(text, top_k=3):
        print(f"- Index {r['index']}, Label: {r['label']}, Score: {r['score']}, Distance: {r['distance']:.4f}")
        print(f"  Teks: {r['text']}")
while True:
    ulasan = input("\nMasukkan ulasan (atau ketik 'exit' untuk berhenti): ")
    if ulasan.lower() == "exit":
        break
    predict_and_print(ulasan)